title: SUMOSIM training
description: |
  Sumosim is a simulation for sumo robots. Two robots are
  placed on a circular field. Each of them tries to push
  the opponent out of the field. Winner is who stays longer
  in the field.

  Sumosim is inspired by [Robot-sumo](https://en.wikipedia.org/wiki/Robot-sumo)
methods:
- title: Q-Learning
  abstract: Use Q-learning to train sumo agents
  description: |
    Q-learning is a model-free reinforcement learning algorithm
    that teaches an agent to assign values to each action it
    might take, conditioned on the agent being in a
    particular state. It does not require a model of the environment
    (hence "model-free"), and it can handle problems with stochastic
    transitions and rewards without requiring adaptations. [wiki]
  trainings:
  - prefix: Q-CV4
    title: Cross validation on all Parameters 20k epochs
    color: GreenYellow
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-cross-0
    description: |
      number of epochs: 20k

      Rewards are limited by a lower and upper limit. This might cause problems.
  - prefix: Q-CV5
    title: Cross validation reward unlimited
    color: GreenYellow
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-cross-1
    description: |
      number of epochs: 20k

      Rewards are no longer limited by a lower and upper limit.continuous-consider-all

      The ranges for learning rate, epsilon and discount were chosen
      from the results of Q-CV4
  - prefix: Q-CV7
    title: Cross validation reward at end
    color: GreenYellow
    enumdescs:
      - training.simrunner.RewardHandlerName.end-consider-all
      - training.parallel.ParallelConfig.q-cross-1
    description: |
      number of epochs: 20k

      Rewards are no longer limited by a lower and upper limit.

      Rewards are only calculated at the end of a simulation. Rewards on intermediate
      steps are zero.

      The ranges for learning rate, epsilon and discount were chosen
      from the results of Q-CV4
  - prefix: QMAP01
    title: Cross validation mapping
    color: LightPink
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-map-0
    description: |
      number of epochs: 20k

      Rewards are no longer limited by a lower and upper limit.

      The ranges for learning rate, epsilon and discount were chosen
      from the results of Q-CV4

      All values for mapping are used.
  - prefix: QMAP02
    title: Cross validation mapping
    color: LightPink
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-map-1
    description: |
      number of epochs: 20k

      Rewards are no longer limited by a lower and upper limit.

      For the training start performance see: [Training start performance](analysis/StartPerformance.html)
  - prefix: QMAP03
    title: Cross validation mapping 2k epochs
    color: LightPink
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-map-2
    description: |
      number of epochs: 2k
      Based on the results of QMAP02
  - prefix: QMAP04
    title: Cross validation mapping 10k epochs
    color: LightPink
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-map-2
    description: |
      number of epochs: 10k
      Based on the results of QMAP02
  - prefix: QMAP05
    title: Cross validation mapping 2k epochs
    color: LightPink
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-map-3
    description: |
      number of epochs: 2k
      Based on the results of QMAP03 and QMAP04
  - prefix: QMAP06
    title: Cross validation mapping 5k epochs
    color: LightPink
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-map-3
    description: |
      number of epochs: 5k
      Based on the results of QMAP03 and QMAP04
  - prefix: QMAP08
    title: Check variance of equal parameters
    color: LightPink
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-map-4
    description: |
      Run multiple trainings with equal parameters explore the variance 
      of training results.

    epoch count: 5000
  - prefix: QRW04
    title: Compare reward handler, reduced push reward
    color: PaleGreen
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.simrunner.RewardHandlerName.reduced-push-reward
      - training.parallel.ParallelConfig.q-rw-0
    description: |
      Run multiple trainings with different reward handlers 
      
      * continuous-consider-all
      * reduced-push-reward

      The agents tended to push opponents as often as possible instead of
      pushing them as soon as possible. Reason was that the sum of rewards
      for just pushing was higher than for winning a game.
      
      The push reward for the 'reduced-push-reward' reward handler
      is 0.1 times the reward for the 'continuous-consider-all' reward handler.

    epoch count: 5000
  - prefix: QRW05
    title: Compare reward handlers with speed-bonus
    color: PaleGreen
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.simrunner.RewardHandlerName.reduced-push-reward
      - training.simrunner.RewardHandlerName.speed-bonus
      - training.parallel.ParallelConfig.q-rw-1
    description: |
      Run multiple trainings with different reward handlers 
      
      * continuous-consider-all
      * reduced-push-reward
      * speed-bonus

      The agent tends to stroll around and collect some extra reward by
      pushing the opponent inside the field. Increasing the 'speed-bonus'
      should reduce that problem.      
      
      The speed bonus was set from 50 tp 150
      
      epoch count: 5000
  - prefix: QRW06
    title: Cross validation for speed-bonus (L,E)
    color: PaleGreen
    enumdescs:
      - training.simrunner.RewardHandlerName.speed-bonus
      - training.parallel.ParallelConfig.q-rw-2
    description: |
      Problem QRW5: Still good solutions get lost after a time when 
      using the 'speed-bonus' reward handler.
      
      Goal: Find out if a more constant increase can be found

      Run multiple trainings with the 'speed-bonus' reward handler and cross 
      validate learning rate and epsilon. Use smaller values for
      learning rate and epsilon, as they promise more stability

      epoch count: 10k
  - prefix: QRW07
    title: Cross validation for speed-bonus (L,E) take 2
    color: PaleGreen
    enumdescs:
      - training.simrunner.RewardHandlerName.speed-bonus
      - training.parallel.ParallelConfig.q-rw-3
    description: |
      Problem QRW6: Results still too unstable. 
      
      Goal: Find better values for L and E based on the results of QRW06

      QRW06 shows the best results for L0 and E0. These were the highest
      values for these parameters. For this training we try those values 
      and higher values.

      epoch count: 10k
  - prefix: QLOW00
    title: Very low values for L and E test
    color: Plum
    enumdescs:
      - training.parallel.ParallelConfig.q-low-0
    description: |
      Problem QRW7: Results still too unstable. 
      
      Goal: Find better values for L and E based on the results of QRW07
  - prefix: QLOW03
    title: Very low values for L and E 30k epochs
    color: Plum
    enumdescs:
      - training.parallel.ParallelConfig.q-low-0
    description: |
      Problem QRW7: Results still too unstable. 
      
      Goal: Find better values for L and E based on the results of QRW07

  - prefix: QLOW04
    title: Very low values for L, E 1k epochs
    color: Plum
    enumdescs:
      - training.parallel.ParallelConfig.q-low-1
    description: |
      Problem QLOW03: Results do not change
      
      Goal: Find better values for L and E based on the results of QLOW03
  - prefix: QLOW05
    title: Very low values for L E wit D 1k epochs
    color: Plum
    enumdescs:
      - training.parallel.ParallelConfig.q-low-2
    description: |
      Goal: Find better values for D based on the results of QLOW04

  - prefix: QSEE00
    title: Zero reward for 'seeing' as a baseline 1k epochs
    color: Salmon
    enumdescs:
      - training.parallel.ParallelConfig.q-see-0
    description: |
      Goal: Create a baseline for cross validation of see rewards
      Based on the values from QRW07
  - prefix: QSEE02
    title: Can see. 1k epochs
    color: Salmon
    enumdescs:
      - training.parallel.ParallelConfig.q-see-1
    description: |
      First usage of the 'can-see' reward handler
  - prefix: QSEE03
    title: Can see. 5k epochs
    color: Salmon
    enumdescs:
      - training.parallel.ParallelConfig.q-see-1
    description: |
      Usage of the 'can-see' reward handler
  - prefix: QSEE06
    title: Can see. 20k epochs
    color: DarkOrange
    enumdescs:
      - training.parallel.ParallelConfig.q-see-1
    description: |
      Usage of the 'can-see' reward handler with 20k epochs
  - prefix: QSEE07
    title: Can see. 10k epochs small E
    color: Salmon
    enumdescs:
      - training.parallel.ParallelConfig.q-see-2
    description: |
      Usage of the 'can-see' reward handler with 20k epochs.  
      Small epsilon
  - prefix: QFETCH04
    title: Lazy fetch 20k
    color: Thistle
    enumdescs:
      - training.parallel.ParallelConfig.q-fetch-1
    description: |
      Lazy fetch cross validation. 20k epochs.
  - prefix: QFETCH05
    title: Lazy fetch top values 20k
    color: Thistle
    enumdescs:
      - training.parallel.ParallelConfig.q-fetch-2
    description: |
      Lazy fetch top values cross validation. 20k epochs. 
  - prefix: QED5
    title: Epsilon decay 10k
    color: DarkSeaGreen
    enumdescs:
      - training.parallel.ParallelConfig.q-ed-1
    description: |
      Find the optimal epsilon decay  
  - prefix: QED6
    title: Epsilon decay long decay 10k
    color: DarkSeaGreen
    enumdescs:
      - training.parallel.ParallelConfig.q-ed-2
    description: |
      Find the optimal epsilon decay for decay of 1000 and 3000 epochs  

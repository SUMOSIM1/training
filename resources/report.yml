title: SUMOSIM training
description: |
  Sumosim is a simulation for sumo robots. Two robots are
  placed on a circular field. Each of them tries to push
  the opponent out of the field. Winner is who stays longer
  in the field.

  Sumosim is inspired by [Robot-sumo](https://en.wikipedia.org/wiki/Robot-sumo)
methods:
- title: Q-Learning
  abstract: Use Q-learning to train sumo agents
  description: |
    Q-learning is a model-free reinforcement learning algorithm
    that teaches an agent to assign values to each action it
    might take, conditioned on the agent being in a
    particular state. It does not require a model of the environment
    (hence "model-free"), and it can handle problems with stochastic
    transitions and rewards without requiring adaptations. [wiki]
  trainings:
  - prefix: Q0-DISC0
    title: Cross validation over discount
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-disc-0
    description: |
      Number of epochs: 5000

      Rewards are limited by a lower and upper limit. This might cause problems.
  - prefix: Q0-LR2
    title: Cross validation over learning rate 5k
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-lr-0
    description: |
      Number of epochs: 5000

      Rewards are limited by a lower and upper limit. This might cause problems.
  - prefix: Q0-EPS0
    title: Cross validation over epsilon 5k
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-eps-0
    description: |
      Number of epochs: 5000

      Rewards are limited by a lower and upper limit. This might cause problems.
  - prefix: Q0-DISC3
    title: Cross validation over discount 2
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-disc-1
    description: |
      Number of epochs: 10000

      Rewards are limited by a lower and upper limit. This might cause problems.
  - prefix: Q0-LR3
    title: Cross validation over learning rate 10k
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-lr-1
    description: |
      Number of epochs: 10000  Q_LR_1

      Rewards are limited by a lower and upper limit. This might cause problems.
  - prefix: Q0-EPS3
    title: Cross validation over epsilon 10k
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-eps-1
    description: |
      Number of epochs: 10000 Q_EPS_1
  - prefix: T-CROSS-1
    title: Cross validation on all Parameters 1 1000 epochs
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
    description: |
      number of epochs 1k

      Rewards are limited by a lower and upper limit. This might cause problems.
  - prefix: T-CROSS2
    title: Cross validation on all Parameters 1k epochs
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-cross-0
    description: |
      Number of epochs 1k

      Rewards are limited by a lower and upper limit. This might cause problems.
  - prefix: Q-CV4
    title: Cross validation on all Parameters 20k epochs
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-cross-0
    description: |
      number of epochs: 20k

      Rewards are limited by a lower and upper limit. This might cause problems.
  - prefix: Q-CV5
    title: Cross validation reward unlimited
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-cross-1
    description: |
      number of epochs: 20k

      Rewards are no longer limited by a lower and upper limit.continuous-consider-all

      The ranges for learning rate, epsilon and discount were chosen
      from the results of Q-CV4
  - prefix: Q-CV7
    title: Cross validation reward at end
    enumdescs:
      - training.simrunner.RewardHandlerName.end-consider-all
      - training.parallel.ParallelConfig.q-cross-1
    description: |
      number of epochs: 20k

      Rewards are no longer limited by a lower and upper limit.

      Rewards are only calculated at the end of a simulation. Rewards on intermediate
      steps are zero.

      The ranges for learning rate, epsilon and discount were chosen
      from the results of Q-CV4
  - prefix: QMAP01
    title: Cross validation mapping
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-map-0
    description: |
      number of epochs: 20k

      Rewards are no longer limited by a lower and upper limit.

      The ranges for learning rate, epsilon and discount were chosen
      from the results of Q-CV4

      All values for mapping are used.
  - prefix: QMAP02
    title: Cross validation mapping
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-map-1
    description: |
      number of epochs: 20k

      Rewards are no longer limited by a lower and upper limit.

      For the training start performance see: [Training start performance](analysis/StartPerformance.html)
  - prefix: QMAP03
    title: Cross validation mapping 2k epochs
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-map-2
    description: |
      number of epochs: 2k
      Based on the results of QMAP02
  - prefix: QMAP04
    title: Cross validation mapping 10k epochs
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-map-2
    description: |
      number of epochs: 10k
      Based on the results of QMAP02
  - prefix: QMAP05
    title: Cross validation mapping 2k epochs
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-map-3
    description: |
      number of epochs: 2k
      Based on the results of QMAP03 and QMAP04
  - prefix: QMAP06
    title: Cross validation mapping 5k epochs
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-map-3
    description: |
      number of epochs: 5k
      Based on the results of QMAP03 and QMAP04
  - prefix: QMAP08
    title: Check variance of equal parameters
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.parallel.ParallelConfig.q-map-4
    description: |
      Run multiple trainings with equal parameters explore the variance 
      of training results.

    epoch count: 5000
  - prefix: QRW04
    title: Compare reward handler, reduced push reward
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.simrunner.RewardHandlerName.reduced-push-reward
      - training.parallel.ParallelConfig.q-rw-0
    description: |
      Run multiple trainings with different reward handlers 
      
      * continuous-consider-all
      * reduced-push-reward

      The agents tended to push opponents as often as possible instead of
      pushing them as soon as possible. Reason was that the sum of rewards
      for just pushing was higher than for winning a game.
      
      The push reward for the 'reduced-push-reward' reward handler
      is 0.1 times the reward for the 'continuous-consider-all' reward handler.

    epoch count: 5000
  - prefix: QRW05
    title: Compare reward handlers with speed-bonus
    enumdescs:
      - training.simrunner.RewardHandlerName.continuous-consider-all
      - training.simrunner.RewardHandlerName.reduced-push-reward
      - training.simrunner.RewardHandlerName.speed-bonus
      - training.parallel.ParallelConfig.q-rw-1
    description: |
      Run multiple trainings with different reward handlers 
      
      * continuous-consider-all
      * reduced-push-reward
      * speed-bonus

      The agent tends to stroll around and collect some extra reward by
      pushing the opponent inside the field. Increasing the 'speed-bonus'
      should reduce that problem.      
      
      The speed bonus was set from 50 tp 150
      
      epoch count: 5000
  - prefix: QRW06
    title: Cross validation for speed-bonus (L,E)
    enumdescs:
      - training.simrunner.RewardHandlerName.speed-bonus
      - training.parallel.ParallelConfig.q-rw-2
    description: |
      Problem QRW5: Still good solutions get lost after a time when 
      using the 'speed-bonus' reward handler.
      
      Goal: Find out if a more constant increase can be found

      Run multiple trainings with the 'speed-bonus' reward handler and cross 
      validate learning rate and epsilon. Use smaller values for
      learning rate and epsilon, as they promise more stability

      epoch count: 10k
  - prefix: QRW07
    title: Cross validation for speed-bonus (L,E) take 2
    enumdescs:
      - training.simrunner.RewardHandlerName.speed-bonus
      - training.parallel.ParallelConfig.q-rw-3
    description: |
      Problem QRW6: Results still too unstable. 
      
      Goal: Find better values for L and E based on the results of QRW06

      QRW06 shows the best results for L0 and E0. These were the highest
      values for these parameters. For this training we try those values 
      and higher values.

      epoch count: 10k
